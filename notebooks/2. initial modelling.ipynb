{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    PowerTransformer,\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    Binarizer\n",
    ")\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/raw/actuarial-loss-estimation/train.csv', parse_dates=['DateTimeOfAccident', 'DateReported'], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['YearAccident'] = df_train['DateTimeOfAccident'].dt.year\n",
    "df_train['DaysToReport'] = (df_train['DateReported'] - df_train['DateTimeOfAccident']).dt.days + 1 # no zero values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transformer = FunctionTransformer(np.log, validate=False)\n",
    "\n",
    "# gender to bool \n",
    "def gender_to_bool(gender_column):\n",
    "    \"\"\"\n",
    "    - Fill missing values with 'M'\n",
    "    - Replace 'U' with 'M'\n",
    "    - Return a boolean-ish column: 1 for 'M', 0 otherwise\n",
    "    \"\"\"\n",
    "    g = pd.Series(gender_column.squeeze(), dtype=str).fillna('M').replace('U', 'M')\n",
    "    is_male = (g == 'M').astype(int)\n",
    "    return is_male.values.reshape(-1, 1)\n",
    "\n",
    "gender_transformer = FunctionTransformer(gender_to_bool, validate=False)\n",
    "\n",
    "# hours worked per week -> to buckets -> to one_hot\n",
    "def bucket_hours_worked(dtt_array):\n",
    "    return pd.cut(\n",
    "        dtt_array.squeeze(), \n",
    "        bins=[-np.inf, 37, 41, np.inf],\n",
    "        labels=[\"<=37\", \"37-41\", \">41\"]\n",
    "    ).astype(str).values.reshape(-1, 1)\n",
    "\n",
    "hours_worked_bucketer = FunctionTransformer(bucket_hours_worked, validate=False)\n",
    "hours_worked_encoder = OneHotEncoder(drop='first')\n",
    "hours_worked_pipeline = Pipeline([\n",
    "    ('bucketizer', hours_worked_bucketer),\n",
    "    ('encoder', hours_worked_encoder)\n",
    "])\n",
    "\n",
    "# DaysToReport (DateReported - DateTimeOfAccident) -> to buckets -> to one_hot\n",
    "def bucket_days_to_report(dtt_array):\n",
    "    return pd.cut(\n",
    "        dtt_array.squeeze(), \n",
    "        bins=[-np.inf, 80, 300, 500, np.inf],\n",
    "        labels=[\"<=80\", \"80-300\", \"300-500\", \">500\"]\n",
    "    ).astype(str).values.reshape(-1, 1)\n",
    "\n",
    "days_to_report_bucketer = FunctionTransformer(bucket_days_to_report, validate=False)\n",
    "days_to_report_encoder = OneHotEncoder(drop='first')\n",
    "days_to_report_pipeline = Pipeline([\n",
    "    ('bucketizer', days_to_report_bucketer),\n",
    "    ('encoder', days_to_report_encoder)\n",
    "])\n",
    "\n",
    "# DaysWorkedPerWeek -> 1 if equals 5, 0 in any other case \n",
    "def days_worked_binarize(days_array):\n",
    "    # Ensure we handle arrays or DataFrames by squeezing to 1D\n",
    "    days = days_array.squeeze()\n",
    "    binarized = (days == 5).astype(int)\n",
    "    # Return as 2D array: (n_samples x 1)\n",
    "    return binarized.values.reshape(-1, 1) if isinstance(days, pd.Series) else binarized.reshape(-1, 1)\n",
    "\n",
    "days_worked_transformer = FunctionTransformer(days_worked_binarize, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('log_inc', log_transformer, ['InitialIncurredCalimsCost', 'WeeklyWages']),\n",
    "        ('minmax_scaler', MinMaxScaler(), ['Age', 'YearAccident']),\n",
    "        ('gender_bool', gender_transformer, ['Gender']),\n",
    "        ('hww_bool_onehot', hours_worked_pipeline, ['HoursWorkedPerWeek']),\n",
    "        ('dtt_bool_onehot', days_to_report_pipeline, ['DaysToReport']),\n",
    "        ('has_dependent_bool', Binarizer(threshold=0), ['DependentChildren']),\n",
    "        ('worked_five_days_bool', days_worked_transformer, ['DaysWorkedPerWeek']),\n",
    "        ('onehot', OneHotEncoder(drop='first'), ['MaritalStatus', 'PartTimeFullTime']),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data fixes \n",
    "\n",
    "# m_weeklywages_under26 = df_train.WeeklyWages <= 26\n",
    "# df_train.loc[m_weeklywages_under26, 'WeeklyWages'] = df_train.loc[~m_weeklywages_under26, 'WeeklyWages'].median()\n",
    "\n",
    "\n",
    "# m_initial_claim_under10 = df_train.InitialIncurredCalimsCost <= 10\n",
    "#df_train.loc[m_initial_claim_under10, 'InitialIncurredCalimsCost'] = df_train.loc[~m_initial_claim_under10, 'InitialIncurredCalimsCost'].median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "X_train = df_train[['InitialIncurredCalimsCost', 'Age', 'Gender', 'DependentChildren', 'MaritalStatus', \n",
    "                    'WeeklyWages', 'PartTimeFullTime', 'HoursWorkedPerWeek',\n",
    "                    'DaysWorkedPerWeek', 'YearAccident', 'DaysToReport']]\n",
    "y_train = df_train['UltimateIncurredClaimCost']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "        experiment_name, \n",
    "        run_name, \n",
    "        regressor_object = LinearRegression(), \n",
    "        kfold = 5, \n",
    "        save_model=True\n",
    "    ):\n",
    "\n",
    "    regressor_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('linear_model', regressor_object)\n",
    "    ])\n",
    "\n",
    "    model = TransformedTargetRegressor(\n",
    "        regressor=regressor_pipeline,\n",
    "        transformer=PowerTransformer(method='box-cox', standardize=False)\n",
    "    )\n",
    "\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # 1) Log relevant hyperparameters or pipeline details\n",
    "        params = model.get_params(deep=True)\n",
    "        for param_name, param_value in params.items():\n",
    "            if isinstance(param_value, (str, int, float, bool, type(None))):\n",
    "                mlflow.log_param(param_name, param_value)\n",
    "            else:\n",
    "                # Convert more complex objects to string\n",
    "                mlflow.log_param(param_name, str(param_value))\n",
    "        \n",
    "        mlflow.log_param(\"n_features_in\", X_train.shape[1])\n",
    "\n",
    "        # Perform cross-validation with predictions\n",
    "        y_pred = cross_val_predict(model, X_train, y_train, cv=kfold)\n",
    "\n",
    "        # Compute overall errors\n",
    "        overall_mse = mean_squared_error(y_train, y_pred)\n",
    "        overall_rmse = np.sqrt(overall_mse)\n",
    "        overall_mae = mean_absolute_error(y_train, y_pred)\n",
    "\n",
    "        # Compute errors for y_true > 100,000\n",
    "        high_mask = y_train > 100000\n",
    "        if high_mask.sum() > 0:  # Ensure there are samples\n",
    "            high_mse = mean_squared_error(y_train[high_mask], y_pred[high_mask])\n",
    "            high_rmse = np.sqrt(high_mse)\n",
    "            high_mae = mean_absolute_error(y_train[high_mask], y_pred[high_mask])\n",
    "        else:\n",
    "            high_mse, high_rmse, high_mae = np.nan, np.nan, np.nan  # No high-value samples\n",
    "\n",
    "        # Compute errors for y_true â‰¤ 100,000\n",
    "        low_mask = y_train <= 100000\n",
    "        if low_mask.sum() > 0:  # Ensure there are samples\n",
    "            low_mse = mean_squared_error(y_train[low_mask], y_pred[low_mask])\n",
    "            low_rmse = np.sqrt(low_mse)\n",
    "            low_mae = mean_absolute_error(y_train[low_mask], y_pred[low_mask])\n",
    "        else:\n",
    "            low_mse, low_rmse, low_mae = np.nan, np.nan, np.nan  # No low-value samples\n",
    "\n",
    "\n",
    "        \n",
    "        # 2) Perform cross-val\n",
    "        scores = cross_val_score(\n",
    "            model, \n",
    "            X_train, \n",
    "            y_train, \n",
    "            cv=kfold, \n",
    "            scoring='neg_mean_squared_error'\n",
    "        )\n",
    "        mse_scores = -scores\n",
    "        rmse_scores = np.sqrt(mse_scores)\n",
    "\n",
    "        print(f\"CV MSE:  {mse_scores.mean():.3f}  (+/- {mse_scores.std():.3f})\")\n",
    "        print(f\"CV RMSE: {rmse_scores.mean():.3f}  (+/- {rmse_scores.std():.3f})\")\n",
    "\n",
    "        # 3) Log metrics\n",
    "        mlflow.log_metric(\"cv_mse\", mse_scores.mean())\n",
    "        mlflow.log_metric(\"cv_rmse\", rmse_scores.mean())\n",
    "\n",
    "        # 4) Fit the final model on the full train set\n",
    "        # 5) Log the fitted pipeline as an MLflow artifact\n",
    "        if save_model:\n",
    "            model.fit(X_train, y_train)\n",
    "            mlflow.sklearn.log_model(model, artifact_path=\"models\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MSE:  1176452243.961  (+/- 533261087.004)\n",
      "CV RMSE: 33602.085  (+/- 6881.287)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/02/25 09:51:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MSE:  867841643.297  (+/- 529464483.964)\n",
      "CV RMSE: 28424.890  (+/- 7737.393)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/02/25 09:51:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 617\n",
      "[LightGBM] [Info] Number of data points in the train set: 43200, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 5.903592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexis/Projects/202502 Actuarial Loss Prediction/.env/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 617\n",
      "[LightGBM] [Info] Number of data points in the train set: 43200, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 5.914297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexis/Projects/202502 Actuarial Loss Prediction/.env/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 617\n",
      "[LightGBM] [Info] Number of data points in the train set: 43200, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 5.938893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexis/Projects/202502 Actuarial Loss Prediction/.env/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 616\n",
      "[LightGBM] [Info] Number of data points in the train set: 43200, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 5.953937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexis/Projects/202502 Actuarial Loss Prediction/.env/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 616\n",
      "[LightGBM] [Info] Number of data points in the train set: 43200, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 5.933814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexis/Projects/202502 Actuarial Loss Prediction/.env/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MSE:  866981745.801  (+/- 529081345.265)\n",
      "CV RMSE: 28410.596  (+/- 7734.326)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 617\n",
      "[LightGBM] [Info] Number of data points in the train set: 54000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 5.928804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/02/25 09:52:05 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "n_vars = X_train.shape[1]\n",
    "\n",
    "run_experiment(\n",
    "    'Actuarial Loss Prediction - initial modelling',\n",
    "    f'Lasso - {n_vars} var',\n",
    "    Lasso()\n",
    ")\n",
    "\n",
    "run_experiment(\n",
    "    'Actuarial Loss Prediction - initial modelling',\n",
    "    f'XGBoost - {n_vars} var',\n",
    "    XGBRegressor()\n",
    ")\n",
    "\n",
    "run_experiment(\n",
    "    'Actuarial Loss Prediction - initial modelling',\n",
    "    f'LightGBM - {n_vars} var',\n",
    "    LGBMRegressor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vars = X_train.shape[1]\n",
    "\n",
    "run_experiment(\n",
    "    'Actuarial Loss Prediction - initial modelling - datafix',\n",
    "    f'Lasso - {n_vars} var - weeklywages, initialclaim NO FIX',\n",
    "    Lasso()\n",
    ")\n",
    "\n",
    "run_experiment(\n",
    "    'Actuarial Loss Prediction - initial modelling - datafix',\n",
    "    f'DT - {n_vars} var - weeklywages, initialclaim NO FIX',\n",
    "    DecisionTreeRegressor()\n",
    ")\n",
    "\n",
    "run_experiment(\n",
    "    'Actuarial Loss Prediction - initial modelling - datafix',\n",
    "    f'RF - {n_vars} var - weeklywages, initialclaim NO FIX',\n",
    "    RandomForestRegressor()\n",
    ")\n",
    "\n",
    "run_experiment(\n",
    "    'Actuarial Loss Prediction - initial modelling - datafix',\n",
    "    f'GB - {n_vars} var - weeklywages, initialclaim NO FIX',\n",
    "    GradientBoostingRegressor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll run a first model using only the InitialIncurredCalimsCost, that will work as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxcox_transformer = PowerTransformer(method=\"box-cox\", standardize=False)\n",
    "y_train_transformed = boxcox_transformer.fit_transform(df_train[['UltimateIncurredClaimCost']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_boxcox(y_transformed, lmbda): \n",
    "    y_mean = np.mean(y_transformed)\n",
    "    y_std = np.std(y_transformed)\n",
    "\n",
    "    y_unscaled = (y_transformed * y_std) + y_mean\n",
    "\n",
    "    return sp.special.inv_boxcox(y_unscaled, lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(\n",
    "    np.log(df_train[['InitialIncurredCalimsCost']]), \n",
    "    y_train_transformed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_transformed = lr.predict(np.log(df_train[['InitialIncurredCalimsCost']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sp.special.inv_boxcox(y_pred_transformed, boxcox_transformer.lambdas_[0]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(df_train['UltimateIncurredClaimCost'], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = inverse_boxcox(y_pred_transformed, boxcox_transformer.lambdas_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(df_train['UltimateIncurredClaimCost'], y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
